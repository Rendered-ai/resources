{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Learning Objectives\n",
    " \n",
    "Explore how changing data generation parameters can reveal variance in an industry grade computer vision object detecion model.\n",
    "\n",
    "1. Sign up with the AI-model-adaptation platform, NVIDIA Train, Adapt, Optimize (TAO) and Rendered.ai's data generation engine, Ana.\n",
    "2. Use a generated dataset to train a ResNet-18 Faster R-CNN model <br>\n",
    "3. Evaluate several datasets with the model to quantify the sensitivity of object detection accuaracy to object obstruction.\n",
    "\n",
    "Supporting Resources:<br>\n",
    "* https://rendered.ai/<br>\n",
    "* https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/fasterrcnn.htm<br>\n",
    "* https://docs.nvidia.com/metropolis/TLT/tlt-user-guide/text/object_detection/fasterrcnn.html\n",
    "\n",
    "\n",
    " ### Table of Contents\n",
    " 1. [Virtual Environment](#head-1)<br>\n",
    " 2. [Configure TAO](#head-2)<br>\n",
    "    2.2 [Evironment Variables](#head-2-2)\n",
    " 3. [Download RenderedAI Datasets](#head-3)<br>\n",
    "     3.1 [Authenticate with Rendered AI SDK](#head-3-1)<br>\n",
    "     3.2 [Download and Verify Dataset and kitti format annotations](#head-3-2)<br>\n",
    " 4. [Prepare dataset and pretrained model](#head-4)<br>\n",
    "     4.1 [Prepare tfrecords from kitti format dataset](#head-4-1)<br>\n",
    "     4.2 [Download pretrained model](#head-4-2)\n",
    " 5. [Run TAO training](#head-5) <br>\n",
    "     5.1 [Provide training specification](#head-5-1)\n",
    " 6. [Evaluate trained models](#head-6)\n",
    " 7. [Visualize inferences](#head-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Environment for Local Experiment <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "Before you start, install Docker and NVIDIA drivers:\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 460+\n",
    "\n",
    "The Python environment should be based on Python 3.7 and this notebook can be loaded in Jupyer-Lab on OSx (M1 chip not supported), Windows, or Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to change the kernel of the notebook to your virtual environment\n",
    "import sys\n",
    "if sys.version_info[1]!=7:\n",
    "    raise Exception(\"Must be Python 3.7, got {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a site-package in the local directory for python packages\n",
    "import os\n",
    "!mkdir site-packages\n",
    "\n",
    "currentDirList = !pwd\n",
    "currentDir = currentDirList[0]\n",
    "currentPathsList = !echo $PATH\n",
    "currentPaths = currentPathsList[0]\n",
    "sitePackagesDir = os.path.join(currentDir, 'site-packages')\n",
    "newPaths = ':'.join([currentPaths[0], sitePackagesDir, os.path.join(sitePackagesDir, 'bin')])\n",
    "%env PATH={newPaths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Rendered.ai SDK `anatools` and other dependencies for this notebook\n",
    "if not os.path.isdir(os.path.join(sitePackagesDir, 'anatools')):\n",
    "    !{sys.executable} -m pip install anatools -t site-packages\n",
    "    !{sys.executable} -m pip install tensorflow -t site-packages\n",
    "    !{sys.executable} -m pip install matplotlib==3.3.3 -t site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Install the TAO Launcher <a class=\"anchor\" id=\"head-2\"></a>\n",
    "TAO interfaces with NGC by running a Docker locally that the tao command uses to prepare data, download pretrained\n",
    "models, train models, etc. The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Skip this step if you have already installed the tao launcher.\n",
    "if not os.path.isdir(os.path.join(sitePackagesDir, 'bin')):\n",
    "    !{sys.executable} -m pip install nvidia-pyindex -t site-packages\n",
    "    !{sys.executable} -m pip install nvidia-tao -t site-packages\n",
    "\n",
    "# View the version of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure the local environment to use TAO <a class=\"anchor\" id=\"head-2-2\"></a>\n",
    "\n",
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. For more information please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/text/tao_launcher.html) in the user guide.\n",
    "\n",
    "When running this cell on AWS, update the drive_map entry with the dictionary defined below, so that you don't have permission issues when writing data into folders created by the TAO docker.\n",
    "\n",
    "```json\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"TAO_SPECS_DIR\"]\n",
    "        },\n",
    "    ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"1000:1000\"\n",
    "    },\n",
    "    # set gpu index for tao-converter\n",
    "    \"Envs\": [\n",
    "        {\"variable\": \"CUDA_VISIBLE_DEVICES\", \"value\": os.getenv(\"GPU_INDEX\")},\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following global environment variables identify the location of the project and the data. Note the `$LOCAL_PROJECT_DIR` is the path to the local workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Please replace the variables with your own.\")\n",
    "%env GPU_INDEX=0\n",
    "%env KEY=renderedai\n",
    "\n",
    "os.environ[\"USR\"] = str(os.getuid())  # use '1000' for windows\n",
    "os.environ[\"GRP\"] = str(os.getgid())  # use '1000' for windows\n",
    "\n",
    "projectDir = os.path.abspath(os.path.join(os.getcwd(), '..') )  # 'local/path/to/tao_experiments'\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = projectDir\n",
    "!echo $LOCAL_PROJECT_DIR\n",
    "\n",
    "experimentDir = os.getcwd()\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = experimentDir\n",
    "\n",
    "experimentName =  os.path.basename(experimentDir)\n",
    "\n",
    "renderedDataDir =  'data_rendered'\n",
    "os.environ['LOCAL_DATA_DIR'] = os.path.join(projectDir, renderedDataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "\n",
    "taoDockerProjectDir = \"/workspace/tao-experiments/\"\n",
    "taoDockerExperimentDir = taoDockerProjectDir + experimentName\n",
    "taoDockerDataDir = taoDockerProjectDir + renderedDataDir\n",
    "os.environ[\"TAO_DATA_DIR\"]=taoDockerDataDir\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "# Set this path if you don't run the notebook from the samples directory.  \n",
    "localSepcsDir = os.path.join(experimentDir, \"specs\")\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = localSepcsDir\n",
    "os.environ[\"TAO_SPECS_DIR\"] = os.path.join(taoDockerExperimentDir, 'specs')\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR\n",
    "\n",
    "# Set the MPLCONFIGDIR environment variable to a writable directory to speed up the import of Matplotlib and to better support multiprocessing.\n",
    "os.environ[\"TAO_MPLCONFIG_DIR\"] = os.path.join(taoDockerExperimentDir, \"matplotlib_config\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": taoDockerProjectDir\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"TAO_SPECS_DIR\"]\n",
    "        },\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_DATA_DIR\"],\n",
    "            \"destination\": os.environ[\"TAO_DATA_DIR\"]\n",
    "        }\n",
    "    ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"{}:{}\".format(os.environ[\"USR\"], os.environ[\"GRP\"])\n",
    "    },\n",
    "    # set gpu index for tao-converter\n",
    "    \"Envs\": [\n",
    "        {\"variable\": \"CUDA_VISIBLE_DEVICES\", \"value\": os.getenv(\"GPU_INDEX\")},\n",
    "        {\"variable\": \"MPLCONFIGDIR\", \"value\": os.getenv(\"TAO_MPLCONFIG_DIR\")}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Writing the mounts file.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the state of your mounts file\n",
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Download Training Dataset from Rendered.ai <a class=\"anchor\" id=\"head-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Authenticate with Rendered using the anatools SDK <a class=\"anchor\" id=\"head-3-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import anatools\n",
    "ana = anatools.client()\n",
    "# Log in to Rendered.ai; Register at https://www.rendered.ai/waitlist.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a workspace\n",
    "try:\n",
    "    workspacesByName={ws['name']:ws for ws in ana.get_workspaces()}\n",
    "except AttributeError:\n",
    "    print(\"Login Expired\")\n",
    "    ana = anatools.client()\n",
    "    workspacesByName={ws['name']:ws for ws in ana.get_workspaces()}\n",
    "\n",
    "workspace = workspacesByName['NVIDIA TAO']  ### CHANGE THIS FOR YOUR WORKSPACE IN YOUR ACCOUNT\n",
    "print(workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a dataset for training tao and add the name to the environment\n",
    "datasets = ana.get_datasets(workspaceId=workspace['workspaceId'])\n",
    "\n",
    "datasetsByName = {ds['name']:ds for ds in datasets}\n",
    "print('DATASET NAME - DESCRIPTION')\n",
    "for n,ds in datasetsByName.items():\n",
    "    print('{} - {}'.format(n, ds['description']))\n",
    "\n",
    "datasetName = '1000runs_15objectScene'\n",
    "os.environ['DATASET_NAME'] = datasetName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendered.ai supports various annotation formats including COCO and KITTI. These are application specific annotation formats that can be generated with `anatools`. To integrate with NVIDIA TAO faster_rcnn, here we use the KITTI annotaion format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KITTI label generation utility\n",
    "def createKittiLabels(mapfilepath, dataset_name = os.environ['DATASET_NAME']):\n",
    "    datasetdir = os.path.join(os.environ['LOCAL_DATA_DIR'], dataset_name, 'output')\n",
    "    kittiDir = os.path.join(os.environ['LOCAL_DATA_DIR'], dataset_name, 'output', 'kitti_labels')\n",
    "    if not os.path.isdir(kittiDir):\n",
    "        os.mkdir(kittiDir)\n",
    "    anatools.Annotations().dump_kitti(datasetdir, kittiDir, mapfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset and the default object mapping files\n",
    "# Other availailable mappings can be used: https://github.com/Rendered-ai/ana/tree/main/ana/channels/example/mappings\n",
    "mappingsDir = os.path.join(os.environ['LOCAL_DATA_DIR'], 'mappings')\n",
    "mappingsFileName = os.path.join(mappingsDir, 'default.yml')\n",
    "if os.path.isfile(mappingsFileName):\n",
    "    print(\"Found mapping file \" + mappingsFileName)\n",
    "else:\n",
    "    wget_command = '-P {} https://raw.githubusercontent.com/Rendered-ai/ana/main/ana/channels/example/mappings/default.yml'.format(mappingsDir)\n",
    "    !wget {wget_command}\n",
    "    print(\"Downloaded \" + mappingsFileName)\n",
    "\n",
    "dataDir = os.path.join(os.environ['LOCAL_DATA_DIR'], os.environ['DATASET_NAME'])\n",
    "if os.path.isdir(dataDir):\n",
    "    print(\"Found dataset \" + datasetName)\n",
    "else:\n",
    "    print(\"This might take some time...\")\n",
    "    print(\"Downloading \" + dataDir)\n",
    "    datasetZipFile = ana.download_dataset(datasetsByName[datasetName]['datasetId'], workspace['workspaceId'])\n",
    "    \n",
    "    print(\"Unzipping...\")\n",
    "    !mkdir -p $dataDir\n",
    "    !unzip -q -u $datasetZipFile -d $dataDir\n",
    "    \n",
    "    # Check the dataset is present\n",
    "    unzipSuccess = os.path.isdir(os.path.join(os.environ['LOCAL_DATA_DIR'], os.environ['DATASET_NAME'], 'output', 'images'))\n",
    "    if unzipSuccess:\n",
    "        os.remove(datasetZipFile)\n",
    "    \n",
    "    createKittiLabels(matppingsFileName, datasetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verify Dataset <a class=\"anchor\" id=\"head-3-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Number of Images\n",
    "num_images = len(os.listdir(os.path.join(dataDir, 'output', 'images')))\n",
    "print(\"Number of images in the dataset. {}\".format(num_images))\n",
    "\n",
    "num_labels = len(os.listdir(os.path.join(dataDir, 'output', 'kitti_labels')))\n",
    "print(\"Number of labels in the dataset. {}\\n\".format(num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Verify KITTI annotations\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sample kitti labels.\n",
    "filename = os.listdir(os.path.join(os.environ['LOCAL_DATA_DIR'], os.environ['DATASET_NAME'], 'output', 'kitti_labels'))[0]\n",
    "print(filename)\n",
    "with open(os.path.join(os.environ['LOCAL_DATA_DIR'], os.environ['DATASET_NAME'], 'output', 'kitti_labels', filename)) as fin:\n",
    "    for i in range(5):\n",
    "        print(fin.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Prepare dataset and pretrained model <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare tfrecords from kitti format dataset <a class=\"anchor\" id=\"head-4-1\"></a>\n",
    "\n",
    "* Update the template tfrecords spec file to take in the synthetic kitti format dataset\n",
    "* Create the tfrecords using the dataset_convert\n",
    "* TFRecords only need to be generated once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fin = open(os.path.join(localSepcsDir, 'frcnn_tfrecords_trainval.txt'))\n",
    "with open(os.path.join(localSepcsDir, 'frcnn_tfrecords_trainval_tmp.txt'), 'w') as fout:\n",
    "    for line in fin:\n",
    "        fout.write(line.replace('datasetName', os.environ['DATASET_NAME']))\n",
    "fin.close()\n",
    "\n",
    "print(\"TFrecords conversion spec file for training\")\n",
    "!cat $LOCAL_SPECS_DIR/frcnn_tfrecords_trainval_tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn dataset_convert \\\n",
    "  --gpu_index $GPU_INDEX \\\n",
    "  -d $TAO_SPECS_DIR/frcnn_tfrecords_trainval_tmp.txt \\\n",
    "  -o $TAO_DATA_DIR/$DATASET_NAME/output/tfrecords/trainval\n",
    "\n",
    "# View the TF records directory\n",
    "#!ls -rlt $LOCAL_DATA_DIR/$DATASET_NAME/output/tfrecords | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at a tfrecord\n",
    "import tensorflow as tf\n",
    "\n",
    "tfrecordDir = os.path.join(os.environ['LOCAL_DATA_DIR'], os.environ['DATASET_NAME'], 'output', 'tfrecords')\n",
    "tfrecord = None\n",
    "for tfrecordfile in os.listdir(tfrecordDir):\n",
    "    print(tfrecordfile)\n",
    "    for example in tf.compat.v1.python_io.tf_record_iterator(os.path.join(tfrecordDir, tfrecordfile)):\n",
    "        tfrecord = tf.train.Example.FromString(example)\n",
    "        print(tfrecord)\n",
    "    if tfrecord:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.2 Download pre-trained model <a class=\"anchor\" id=\"head-4-2\"></a>\n",
    "\n",
    "To get the ResNet-18 Backbone from NGC. Please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Install NGC CLI on the local machine.\n",
    "\n",
    "## Download and install\n",
    "if os.path.isdir(os.path.join(projectDir, 'ngccli')):\n",
    "    print(\"Found existing model \" + datasetName)\n",
    "    !rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "else:\n",
    "    print(\"Downloading...\")\n",
    "    %env CLI=ngccli_cat_linux.zip\n",
    "    !mkdir -p $LOCAL_PROJECT_DIR/ngccli \n",
    "    !wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "    !unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "    !rm $LOCAL_PROJECT_DIR/ngccli/*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"]=\"{}/ngccli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n",
    "!ngc registry model list nvidia/*pretrained_object_detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download model from NGC.\n",
    "!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Verify ResNet-18 download\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/pretrained_object_detection_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Run TAO Training <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5.1. Provide training specification <a class=\"anchor\" id=\"head-5-1\"></a>\n",
    " The NVIDIA TAO commands used throughout this notebook use the spec files in $LOCAL_SPECS_DIR. The following helper funciton clones the file used for training, evaluation and making inferences. The parameters within are described in the [TAO Toolkit Documentation](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/detectnet_v2.html#creating-a-configuration-file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the tao model file to use the resnet18 model to detect all the objects as individual classes.\n",
    "def experimentSpec():\n",
    "    fin = open(os.path.join(localSepcsDir, 'alltoys_spec_resnet18.txt'))\n",
    "    with open(os.path.join(localSepcsDir, 'alltoys_spec_resnet18_tmp.txt'), 'w') as fout:\n",
    "        for line in fin:\n",
    "            fout.write(line.replace('$KEY', os.environ['KEY']).replace('datasetName', os.environ['DATASET_NAME']))\n",
    "    fin.close()\n",
    "\n",
    "experimentSpec()\n",
    "\n",
    "!cat $LOCAL_SPECS_DIR/alltoys_spec_resnet18_tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You can restart with the last completed epoch: uncomment line 104 in the spec file\n",
    "!tao faster_rcnn train --gpu_index $GPU_INDEX -e $TAO_SPECS_DIR/alltoys_spec_resnet18_tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"For resume training from checkpoint, please uncomment and run this instead. Change/Add the 'resume_from_model' field in the spec file.\")\n",
    "#!tao faster_rcnn train --gpu_index $GPU_INDEX -e $TAO_SPECS_DIR/alltoys_spec_resnet18_tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -lht *$DATASET_NAME*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Evaluate Model on Various Datasets <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation sets \n",
    "    \n",
    "# Look at the datasets in your workspace\n",
    "for ds in datasets:\n",
    "    print('{}: {}'.format(ds['name'],ds['description']))\n",
    "\n",
    "# Choose test sets\n",
    "datasetNameObstruction10 = '200runs_10objectScene'\n",
    "datasetNameObstruction40 = '200runs_40objectScene'\n",
    "datasetNameObstruction70 = '200runs_70objectScene'\n",
    "datasetNameObstruction100 = '200runs_100objectScene'\n",
    "datasetNameObstruction200 = '200runs_200objectScene'\n",
    "\n",
    "testDatasets = [datasetNameObstruction10, datasetNameObstruction40, datasetNameObstruction70, datasetNameObstruction100]\n",
    "\n",
    "print('\\nUsing the following datasets...')\n",
    "for testDataset in testDatasets:\n",
    "    print('{}: {}\\n'.format(testDataset, datasetsByName[testDataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Test Datasets\n",
    "for testDataset in testDatasets:\n",
    "    dataDir = os.path.join(projectDir, renderedDataDir, testDataset)\n",
    "    if os.path.isdir(dataDir):\n",
    "        print(\"Found dataset \" + testDataset)\n",
    "    else:\n",
    "        print(\"Downloading \" + dataDir)\n",
    "        try:\n",
    "            datasetZipFile = ana.download_dataset(datasetsByName[testDataset]['datasetId'], workspace['workspaceId'])\n",
    "        except AttributeError:\n",
    "            print(\"Login Expired\")\n",
    "            ana = anatools.client()\n",
    "            print(\"Downloading \" + dataDir)\n",
    "            datasetZipFile = ana.download_dataset(datasetsByName[testDataset]['datasetId'], workspace['workspaceId'])\n",
    "\n",
    "        print(\"Unzipping ...\")\n",
    "        !mkdir -p $dataDir\n",
    "        !unzip -q -u $datasetZipFile -d $dataDir\n",
    "\n",
    "        # Check the dataset is present\n",
    "        unzipSuccess = os.path.isdir(os.path.join(os.environ['LOCAL_DATA_DIR'], testDataset, 'output', 'images'))\n",
    "        if unzipSuccess:\n",
    "            os.remove(datasetZipFile)\n",
    "\n",
    "        createKittiLabels(mappingsFileName, testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Verify test sets\n",
    "for testDataset in testDatasets:\n",
    "    print(testDataset)\n",
    "    dataDir = os.path.join(os.environ['LOCAL_DATA_DIR'], testDataset)\n",
    "    num_images = len(os.listdir(os.path.join(dataDir, 'output', 'images')))\n",
    "    print(\"Number of images in the dataset. {}\".format(num_images))\n",
    "\n",
    "    #Kitti Labels\n",
    "    num_labels = len(os.listdir(os.path.join(dataDir, 'output', 'kitti_labels')))\n",
    "    print(\"Number of labels in the dataset. {}\\n\".format(num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate tensorflow records\n",
    "for testDataset in testDatasets:\n",
    "    print(\"Createing TFrecords for test set: \" + testDataset)\n",
    "    fin = open(os.path.join(localSepcsDir, 'frcnn_tfrecords_eval.txt'))\n",
    "    with open(os.path.join(localSepcsDir, 'frcnn_tfrecords_eval_tmp.txt'), 'w') as fout:\n",
    "        for line in fin:\n",
    "            fout.write(line.replace('datasetName', testDataset))\n",
    "    fin.close()\n",
    "\n",
    "    relativeDatasetDir =  os.path.join('data_rendered', testDataset, \"output\")\n",
    "    os.environ[\"TAO_TESTSET_DIR\"] = os.path.join(taoDockerProjectDir, relativeDatasetDir)\n",
    "\n",
    "    !tao faster_rcnn dataset_convert --gpu_index $GPU_INDEX \\\n",
    "       -d $TAO_SPECS_DIR/frcnn_tfrecords_eval_tmp.txt -o $TAO_TESTSET_DIR/tfrecords/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the experiment spec file to use the evaluation images and tfrecords in dataset_config[data_sources]\n",
    "def evalModel(testDataset):\n",
    "    # Reset the experiment spec so the dataset_config parameters are for the training name ('datsetName')\n",
    "    experimentSpec()\n",
    "    with open(os.path.join(os.environ[\"LOCAL_SPECS_DIR\"], 'alltoys_spec_resnet18_tmp.txt')) as fin:\n",
    "        filedata = fin.read()\n",
    "\n",
    "    existingdata = '{}/output'.format(datasetName)\n",
    "    newdata = '{}/output'.format(testDataset)\n",
    "    filedata = filedata.replace(existingdata, newdata)\n",
    "\n",
    "    with open(os.path.join(os.environ[\"LOCAL_SPECS_DIR\"], 'alltoys_spec_resnet18_tmp.txt'), 'w') as fout:\n",
    "        fout.write(filedata)\n",
    "\n",
    "    !tao faster_rcnn evaluate --gpu_index $GPU_INDEX -e $TAO_SPECS_DIR/alltoys_spec_resnet18_tmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_obstruction(kitti_dir):\n",
    "    truncation = 0\n",
    "    occlusion = 0\n",
    "    nobjects = 0\n",
    "    for kittifilename in os.listdir(kitti_dir):\n",
    "        with open(os.path.join(kitti_dir, kittifilename), 'r') as af:\n",
    "            labels = af.readlines()\n",
    "\n",
    "        for label in labels:\n",
    "            features = label.split(' ')\n",
    "            truncation += int(features[1])\n",
    "            occlusion += int(features[2])\n",
    "        nobjects += len(labels)\n",
    "    obstruction = (truncation + occlusion)/nobjects\n",
    "    return obstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = testDatasets[0]\n",
    "evalModel(testDataset)\n",
    "kittiDir = os.path.join(projectDir, renderedDataDir, testDataset, 'output', 'kitti_labels')\n",
    "print(\"\\nAverage Object Obstruction: {}\".format(estimate_obstruction(kittiDir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = testDatasets[1]\n",
    "evalModel(testDataset)\n",
    "kittiDir = os.path.join(projectDir, renderedDataDir, testDataset, 'output', 'kitti_labels')\n",
    "print(\"\\nAverage Object Obstruction: {}\".format(estimate_obstruction(kittiDir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = testDatasets[2]\n",
    "evalModel(testDataset)\n",
    "kittiDir = os.path.join(projectDir, renderedDataDir, testDataset, 'output', 'kitti_labels')\n",
    "print(\"\\nAverage Object Obstruction: {}\".format(estimate_obstruction(kittiDir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = testDatasets[3]\n",
    "evalModel(testDataset)\n",
    "kittiDir = os.path.join(projectDir, renderedDataDir, testDataset, 'output', 'kitti_labels')\n",
    "print(\"\\nAverage Object Obstruction: {}\".format(estimate_obstruction(kittiDir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The following plot of the evaluation results shows the relationship between detection accuracy how much the objects are obstructed. A linear regression analysis indicates mAP0.5 drops 0.78% for each percent obstruction increase.\n",
    "<img align=\"center\" src=\"https://renderedai-public-assets.s3.us-west-2.amazonaws.com/Accuracy_vs_Obstruction.png\" width=\"540\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## 7. Visualize Inferences <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "The `inference` tool of tao faster_rnn generates inferences on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Running inference, see the `inference_config` parameters in the spec file used here.\n",
    "!tao faster_rcnn inference --gpu_index $GPU_INDEX -e $TAO_SPECS_DIR/alltoys_spec_resnet18_tmp.txt\n",
    "print('The inferences are for \"'+testDataset+'\" - see \"inference_config\" of spec file: \"alltoys_spec_resnet18_tmp.txt\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference` tool produces two outputs.\n",
    "1. Overlain images in `$LOCAL_EXPERIMENT_DIR/inference_results_imgs`\n",
    "2. Frame by frame bbox labels in kitti format located in `$LOCAL_EXPERIMENT_DIR/inference_dump_labels`\n",
    "\n",
    "*Note: To run inferences for a single image, simply replace the path to the -i flag in `inference` command with the path to the image.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path)\n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "OUTPUT_PATH = 'inference_results_imgs' # relative path from $LOCAL_EXPERIMENT_DIR.\n",
    "COLS = 3 # number of columns in the visualizer grid.\n",
    "IMAGES = 6 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "!rm $LOCAL_SPECS_DIR/*_tmp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congradulations!\n",
    "Inspecting the bboxes can provide evidence for various hypotheses for further experiments.  For instance the skateboard is the most affeted by obstruction. Perhaps there is one particular style of skateboard that is failing detection or causing false positive. Armed with a hypothesis, the scientist can update the graph on Rendered.ai and add to the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
